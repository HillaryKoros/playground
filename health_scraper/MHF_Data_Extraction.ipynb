{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data from the Master Health Facility Registry (KMHFR)\n",
    "\n",
    "In this exercise, we will use the following Python libraries:\n",
    "\n",
    "- **Beautiful Soup**: For parsing HTML and XML documents.\n",
    "- **Requests**: For making HTTP requests to retrieve data from the web.\n",
    "- **JSON**: For handling JSON data structures.\n",
    "- **Pandas**: For data manipulation and analysis.\n",
    "\n",
    "### About the Master Health Facility Registry (KMHFR)\n",
    "\n",
    "**Master Health Facility Registry (KMHFR)** is an application that contains information on all health facilities and community units in Kenya. Each health facility and community unit is identified with a unique code, along with details describing its geographical location, administrative location, ownership, type, and the services offered.\n",
    "\n",
    "### Accessing the Data\n",
    "\n",
    "The data from KMHFR can be accessed through an API, with documentation available [here](https://mfl-api-docs.readthedocs.io/en/latest/).\n",
    "\n",
    "However, in this exercise, we will use **Beautiful Soup** to scrape the data from the KMHFR website. \n",
    "\n",
    "\n",
    "We'll be working with the following base URL:\n",
    "```python\n",
    "base_url = \"https://kmhfr.health.go.ke/public/facilities?page={}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NB: Ethical Considerations of Web Scraping**\n",
    "\n",
    "Web scraping is a powerful tool for extracting data from websites, but it comes with ethical and legal responsibilities. When scraping data from websites, it is important to:\n",
    "- **Respect the website's robots.txt file**: This file specifies which parts of the site should not be accessed by automated tools. Ignoring this can lead to legal issues or your IP being blocked.\n",
    "- **Avoid overloading the server**: Sending too many requests in a short period can strain the website's server, potentially causing disruptions in service.\n",
    "- **Use data responsibly**: Ensure that the data you scrape is used in accordance with the terms of service of the website, and respect privacy and copyright laws.\n",
    "\n",
    "\n",
    "In this exercise, web scraping is used solely for demonstration purposes. The goal is to illustrate how web scraping works and how data can be extracted and processed using Python libraries. In practice, it is recommended to use official APIs when available, as they are designed to provide access to data in a controlled and ethical manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Used for making HTTP requests to fetch data from websites or APIs\n",
    "\n",
    "from bs4 import BeautifulSoup as bs  # Used for parsing HTML and XML documents, enabling easy extraction of data from web pages\n",
    "\n",
    "import pandas as pd  # Pandas is used for data manipulation and analysis, providing data structures like DataFrames to handle tabular data\n",
    "\n",
    "import json  # JSON library allows for parsing JSON strings and converting Python objects to JSON format\n",
    "\n",
    "import time  # Provides functions for time-related tasks like adding delays or measuring execution time\n",
    "\n",
    "import sys  # Provides access to system-specific parameters and functions for interacting with the Python runtime environment\n",
    "\n",
    "import geopandas as gpd  # Extends pandas to handle geospatial data, enabling operations and analysis on geographic objects\n",
    "\n",
    "from shapely.geometry import Point  # Used for creating and manipulating geometric objects, such as points, lines, and polygons in 2D space\n",
    "\n",
    "import matplotlib.pyplot as plt  # A plotting library used to generate static, animated, and interactive visualizations in Python\n",
    "\n",
    "import folium  # Used for creating interactive maps using Leaflet.js, making it easy to visualize geospatial data\n",
    "\n",
    "from folium.plugins import HeatMap  # Allows for the creation of heat maps to visualize the density of data points on a map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all the url pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the KMHFR data. The placeholder `{}` will be replaced with page numbers.\n",
    "base_url = \"https://kmhfr.health.go.ke/public/facilities?page={}\"\n",
    "\n",
    "# Generate a list of URLs for each page from 1 to 494 .\n",
    "urls = [base_url.format(page) for page in range(1, 495)]\n",
    "\n",
    "# Print the list of generated URLs to verify the URLs created for each page.\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetch json data for each url and combine them to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch JSON data\n",
    "def fetch_json_data(url, headers):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from a given URL and headers.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to fetch data from.\n",
    "        headers (dict): The headers to use for the request.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL with the provided headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Parse the response content using BeautifulSoup\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the script tag containing the JSON data\n",
    "    script_tag = soup.find('script', id='__NEXT_DATA__')\n",
    "    \n",
    "    if script_tag:\n",
    "        # Load and return the JSON data\n",
    "        data = json.loads(script_tag.string)\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "# Function to convert JSON data to DataFrame\n",
    "def json_to_dataframe(json_data):\n",
    "    \"\"\"\n",
    "    Convert JSON data to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        json_data (dict): The JSON data to convert.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    # Extract the results from the JSON data\n",
    "    results = json_data.get('props', {}).get('pageProps', {}).get('data', {}).get('results', [])\n",
    "    \n",
    "    # Convert the results list to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Define the base URL for fetching data, with a placeholder for page numbers\n",
    "base_url = \"https://kmhfr.health.go.ke/public/facilities?page={}\"\n",
    "\n",
    "# Generate a list of URLs for each page from 1 to 494\n",
    "urls = [base_url.format(page) for page in range(1, 495)]\n",
    "\n",
    "# Define headers for HTTP requests to mimic a browser request\n",
    "HEADERS = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9,fr;q=0.8\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to accumulate all the fetched data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each URL and fetch data\n",
    "for url in urls:\n",
    "    data = fetch_json_data(url, HEADERS)\n",
    "    if data:\n",
    "        # Convert JSON data to DataFrame\n",
    "        df = json_to_dataframe(data)\n",
    "        # Append the DataFrame to the accumulated data\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame (number of rows and columns)\n",
    "print(f\"DataFrame shape: {all_data.shape}\")\n",
    "\n",
    "# Print the number of rows in the DataFrame\n",
    "print(f\"Number of rows: {all_data.shape[0]}\")\n",
    "\n",
    "# Print the number of columns in the DataFrame\n",
    "print(f\"Number of columns: {all_data.shape[1]}\")\n",
    "\n",
    "# Print the column names in the DataFrame\n",
    "print(f\"Column names: {all_data.columns.tolist()}\")\n",
    "\n",
    "# Check for null values in each column\n",
    "print(\"\\nNull values per column:\")\n",
    "print(all_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame shape: (14805, 47)\n",
    "Number of rows: 14805\n",
    "Number of columns: 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe to local dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a file\n",
    "all_data.to_csv(\"data/all_facilities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.county_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = all_data.isnull().sum()\n",
    "print(\"Null values per column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.shape[0]\n",
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'lat' or 'long' columns have null values\n",
    "all_data_cleaned = all_data.dropna(subset=['lat', 'long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame by converting latitude and longitude to geometry points\n",
    "geometry = [Point(xy) for xy in zip(all_data_cleaned['lat'], all_data_cleaned['long'])]\n",
    "gdf = gpd.GeoDataFrame(all_data_cleaned, geometry=geometry)\n",
    "\n",
    "# Set the coordinate reference system (CRS) to WGS84 (EPSG:4326)\n",
    "gdf.set_crs(epsg=4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows in the DataFrame\n",
    "print(f\"Number of rows: {gdf.shape[0]}\")\n",
    "gdf.explore(\n",
    "    column='facility_type_category',  # Column to color by\n",
    "    cmap='viridis',            # Colormap\n",
    "    legend=True,               # Show legend\n",
    "    legend_kwds={\n",
    "        'loc': 'topright',     # Position of the legend\n",
    "        'title': 'Keph facility_type_category', # Title of the legend\n",
    "    },\n",
    "    tooltip='keph_level_name', # Tooltip information\n",
    "    style_kwds=dict(radius=4, fillOpacity=0.7)  # Style options\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  drop unneccesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specified columns from the GeoDataFrame\n",
    "#gdf = gdf.drop(columns=['county', 'constituency', 'ward'])\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of columns you want to display in the tooltip\n",
    "tooltip_columns = ['name',  'beds', 'cots', 'search', \n",
    "       'operation_status', 'operation_status_name', 'admission_status_name',\n",
    "       'open_whole_day', 'open_public_holidays', 'open_weekends',\n",
    "       'open_late_night'] \n",
    "\n",
    "# Generate the interactive map with detailed tooltip\n",
    "m = gdf.explore(\n",
    "    column='facility_type_category',  # Column to color by\n",
    "    cmap='viridis',            # Colormap\n",
    "    legend=True,               # Show legend\n",
    "    tooltip=tooltip_columns,   # List of columns to show in the tooltip\n",
    "    style_kwds=dict(radius=4, fillOpacity=0.7)  # Style options\n",
    ")\n",
    "\n",
    "# Save the map as an HTML file\n",
    "m.save('interactive_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
